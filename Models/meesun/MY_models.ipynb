{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#spark = SparkSession.builder.master(\"local\").appName(\"mllib_classifier\").getOrCreate()\n",
    "spark = SparkSession.builder.appName(\"StockPricePrediction\").getOrCreate()\n",
    "\n",
    "# Load training data\n",
    "filename = \"KOv2.csv\"\n",
    "data = spark.read.csv(filename,  inferSchema=True, header = True)\n",
    "#data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.describe of DataFrame[Date: int, Open: double, High: double, Low: double, Close: double, AdjClose: double, Volume: int, MarketCap: double, EnterpriseValue: double, PeRatio: double, PsRatio: double, PbRatio: double, EnterprisesValueRevenueRatio: double, EnterprisesValueEBITDARatio: double, TotalRevenue: bigint, OperatingRevenue: bigint, CostOfRevenue: bigint, GrossProfit: bigint, OperatingExpense: bigint, SellingGeneralAndAdministration: bigint, OperatingIncome: bigint, OtherIncomeExpense: bigint, PretaxIncome: bigint, TaxProvision: bigint, NetIncomeCommonStockholders: bigint, NetIncome: bigint, NetIncomeIncludingNoncontrollingInterests: bigint, NetIncomeContinuousOperations: bigint, DilutedNIAvailtoComStockholders: bigint, BasicEPS: double, DilutedEPS: double, BasicAverageShares: bigint, DilutedAverageShares: bigint, TotalOperatingIncomeAsReported: bigint, TotalExpenses: bigint, NetIncomeFromContinuingAndDiscontinuedOperation: bigint, NormalizedIncome: double, EBIT: bigint, EBITDA: bigint, ReconciledCostOfRevenue: bigint, ReconciledDepreciation: int, NetIncomeFromContinuingOperationNetMinorityInterest: bigint, NormalizedEBITDA: bigint, TaxRateForCalcs: double, TaxEffectOfUnusualItems: double, TotalAssets: bigint, CurrentAssets: bigint, CashCashEquivalentsAndShortTermInvestments: bigint, CashAndCashEquivalents: bigint, Receivables: bigint, Inventory: bigint, TotalNonCurrentAssets: bigint, NetPPE: bigint, GrossPPE: bigint, AccumulatedDepreciation: bigint, GoodwillAndOtherIntangibleAssets: bigint, OtherIntangibleAssets: bigint, OtherNonCurrentAssets: bigint, TotalLiabilitiesNetMinorityInterest: bigint, CurrentLiabilities: bigint, PayablesAndAccruedExpenses: bigint, Payables: bigint, AccountsPayable: bigint, CurrentDebtAndCapitalLeaseObligation: bigint, CurrentDebt: bigint, TotalNonCurrentLiabilitiesNetMinorityInterest: bigint, LongTermDebtAndCapitalLeaseObligation: bigint, LongTermDebt: bigint, NonCurrentDeferredLiabilities: bigint, NonCurrentDeferredTaxesLiabilities: bigint, OtherNonCurrentLiabilities: bigint, TotalEquityGrossMinorityInterest: bigint, StockholdersEquity: bigint, RetainedEarnings: bigint, TotalCapitalization: bigint, CommonStockEquity: bigint, NetTangibleAssets: bigint, WorkingCapital: bigint, InvestedCapital: bigint, TangibleBookValue: bigint, TotalDebt: bigint, ShareIssued: bigint, OrdinarySharesNumber: bigint, OperatingCashFlow: bigint, CashFlowFromContinuingOperatingActivities: bigint, NetIncomeFromContinuingOperations: bigint, DepreciationAmortizationDepletion: int, DepreciationAndAmortization: int, DeferredTax: int, DeferredIncomeTax: int, InvestingCashFlow: bigint, CashFlowFromContinuingInvestingActivities: bigint, NetPPEPurchaseAndSale: int, PurchaseOfPPE: int, FinancingCashFlow: bigint, CashFlowFromContinuingFinancingActivities: bigint, NetIssuancePaymentsOfDebt: bigint, NetCommonStockIssuance: int, CommonStockIssuance: int, CommonStockPayments: bigint, EndCashPosition: bigint, ChangesInCash: bigint, EffectOfExchangeRateChanges: int, BeginningCashPosition: bigint, CapitalExpenditure: int, IssuanceOfCapitalStock: int, RepurchaseOfCapitalStock: bigint, FreeCashFlow: bigint]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/11/21 02:10:36 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "+---------+-------+-------+-------+------------+-----------+\n",
      "| AdjClose|PeRatio|PsRatio|PbRatio|TotalRevenue|GrossProfit|\n",
      "+---------+-------+-------+-------+------------+-----------+\n",
      "|56.490002| 23.228|  5.509|  9.305| 11953000000| 7296000000|\n",
      "|61.442528| 26.529|  6.019| 10.326| 11972000000| 7060000000|\n",
      "|63.164001| 28.324|  6.275|  11.13| 10980000000| 6663000000|\n",
      "|  59.9179| 27.899|  6.537| 12.072| 10125000000| 5612000000|\n",
      "|58.069668| 25.464|  5.898| 10.529| 11063000000| 6497000000|\n",
      "|61.810516| 26.544|  6.815| 10.977| 11325000000| 6495000000|\n",
      "|61.787926| 27.556|  6.961| 11.686| 10491000000| 6400000000|\n",
      "|57.901939| 29.167|   6.79| 11.547|  9464000000| 5376000000|\n",
      "|53.086636| 28.059|  6.239|  10.18| 10042000000| 6065000000|\n",
      "|53.305889| 32.401|  6.999| 11.462| 10129000000| 6342000000|\n",
      "|50.077721| 29.447|  6.902|  11.77|  9020000000| 5515000000|\n",
      "|44.300453| 28.415|   7.08|  12.68|  8611000000| 5033000000|\n",
      "|43.873634| 23.288|  6.216| 12.134|  8652000000| 5181000000|\n",
      "|42.778782| 19.259|  5.191| 10.568| 15751000000| 9367000000|\n",
      "|41.182129| 21.377|  5.122| 10.011|  8601000000| 5230000000|\n",
      "|51.953102| 29.441|  7.108|  12.66|  9068000000| 5502000000|\n",
      "|48.062347| 31.836|  7.135| 12.828|  9507000000| 5740000000|\n",
      "|46.135609| 31.049|  6.788| 12.274|  9997000000| 6076000000|\n",
      "|42.673019| 29.847|  6.324| 11.778|  8020000000| 5030000000|\n",
      "|41.501797| 64.863|  6.304| 11.065|  7058000000| 4337000000|\n",
      "+---------+-------+-------+-------+------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"SubsetColumns\").getOrCreate()\n",
    "\n",
    "# Assuming you have an existing DataFrame called \"original_df\"\n",
    "# Replace \"original_df\" with your actual DataFrame name\n",
    "original_df = data  # Replace this with your actual DataFrame\n",
    "\n",
    "# Define a list of columns you want to include in the subset\n",
    "selected_columns = [\"AdjClose\", \"PeRatio\", \"PsRatio\", \"PbRatio\",\"TotalRevenue\",\"GrossProfit\"]\n",
    "\n",
    "# Use the select function to create a new DataFrame with the subset of columns\n",
    "new_df = original_df.select(selected_columns)\n",
    "\n",
    "# Show the new DataFrame\n",
    "new_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"StringToFloatConversion\").getOrCreate()\n",
    "\n",
    "# Assuming you have a DataFrame called \"your_data\"\n",
    "# Replace \"your_data\" with your actual DataFrame name\n",
    "data = data  # Replace this with your actual DataFrame\n",
    "\n",
    "# Get the list of columns in the DataFrame\n",
    "all_columns = data.columns\n",
    "\n",
    "# Iterate through each column and update its type to float\n",
    "for column in all_columns:\n",
    "    data = data.withColumn(column, col(column).cast(\"float\"))\n",
    "\n",
    "# Show the updated DataFrame\n",
    "data.describe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [0.025478994978851217,3.311269724942866,-0.07299225835558495,4.085880907200866e-09,0.0]\n",
      "Intercept: -24.051203190308343\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "#from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Create a Spark session\n",
    "#spark = SparkSession.builder.appName(\"StockPricePrediction\").getOrCreate()\n",
    "\n",
    "# Assuming your data is in a DataFrame called \"data\"\n",
    "# Replace \"target_column\" with the actual column name of your target variable\n",
    "target_column = 'AdjClose'\n",
    "features_columns = ['PeRatio','PsRatio','PbRatio','TotalRevenue','GrossProfit']\n",
    "\n",
    "# Assemble the features into a vector\n",
    "assembler = VectorAssembler(inputCols=features_columns, outputCol=\"features\")\n",
    "data_assembled = assembler.transform(new_df)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "(train_data, test_data) = data_assembled.randomSplit([0.8, 0.2], seed=123)\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "lr = LinearRegression(labelCol=target_column, featuresCol=\"features\",\n",
    "                       maxIter=10,\n",
    "                      regParam=0.3, \n",
    "                      elasticNetParam=0.8)\n",
    "\n",
    "# Create a pipeline with the assembler and the logistic regression model\n",
    "#pipeline = Pipeline(stages=[lr])\n",
    "\n",
    "# Train the model\n",
    "model = lr.fit(train_data)\n",
    "\n",
    "# Print the weights and intercept for linear regression\n",
    "print(\"Weights: \" + str(model.coefficients))\n",
    "print(\"Intercept: \" + str(model.intercept))\n",
    "\n",
    "# Make predictions on the test set\n",
    "#predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate the model using a binary classification evaluator\n",
    "#evaluator = BinaryClassificationEvaluator(labelCol=target_column)\n",
    "#accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "#print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Optionally, you can print the coefficients of the logistic regression model\n",
    "#print(f\"Coefficients: {model.stages[-1].coefficients}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-------+-------+------------+-----------+--------------------+------------------+\n",
      "| AdjClose|PeRatio|PsRatio|PbRatio|TotalRevenue|GrossProfit|            features|        prediction|\n",
      "+---------+-------+-------+-------+------------+-----------+--------------------+------------------+\n",
      "| 1.957785| 22.991|  3.016|  9.249|  2248900000| 1280400000|[22.991,3.016,9.2...|-4.964993951648665|\n",
      "|   2.6057|  19.81|  2.595|  9.694|  2739300000| 1695200000|[19.81,2.595,9.69...|-4.468852746954269|\n",
      "| 4.719271| 32.788|   4.27| 14.041|  3056000000| 2051000000|[32.788,4.27,14.0...|2.3848915753993474|\n",
      "| 4.833506| 29.348|  4.315| 11.564|  3507500000| 2211200000|[29.348,4.315,11....| 4.471828023842505|\n",
      "| 5.089425| 23.897|  3.761| 10.921|  3352000000| 2201000000|[23.897,3.761,10....|1.9100781356473462|\n",
      "| 6.481538| 27.394|  4.319| 11.724|  4017000000| 2639000000|[27.394,4.319,11....| 6.505364707435547|\n",
      "|11.000153|   27.4|  5.564|   9.18|  4795000000| 3094000000|[27.4,5.564,9.18,...|13.992556040018172|\n",
      "|11.048383| 24.988|  5.023|  8.217|  4498000000| 2896000000|[24.988,5.023,8.2...|10.996488698292865|\n",
      "|11.766837|  20.82|  4.605|  6.296|  5257000000| 3454000000|[20.82,4.605,6.29...|12.747583239061427|\n",
      "|11.804535| 40.329|  6.731| 22.057|  5253000000| 3470000000|[40.329,6.731,22....| 19.11763787976119|\n",
      "+---------+-------+-------+-------+------------+-----------+--------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "--------------------\n",
      "METRICS\n",
      "Mean Squared Error: 43.95498916874773\n",
      "R Squared: 0.8034149757100888\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# compute predictions. this will append column \"prediction\" to dataframe\n",
    "lrPred = model.transform(test_data)\n",
    "lrPred.show(10)\n",
    "\n",
    "ev = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"AdjClose\")\n",
    "\n",
    "print('-'*20)\n",
    "print(\"METRICS\")\n",
    "print(\"Mean Squared Error:\", ev.evaluate(lrPred, {ev.metricName: \"mse\"}))\n",
    "print(\"R Squared:\", ev.evaluate(lrPred, {ev.metricName:'r2'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS5110 Spark 3.3",
   "language": "python",
   "name": "ds5110_spark3.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
