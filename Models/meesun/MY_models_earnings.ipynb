{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#spark = SparkSession.builder.master(\"local\").appName(\"mllib_classifier\").getOrCreate()\n",
    "spark = SparkSession.builder.appName(\"StockPricePrediction\").getOrCreate()\n",
    "\n",
    "# Load training data\n",
    "filename = \"KOv3.csv\"\n",
    "data = spark.read.csv(filename, inferSchema=True, header = True)\n",
    "#data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.describe of DataFrame[Date: string, Open: double, High: double, Low: double, Close: double, Adj Close: double, PercentChangeClose: double, Percent100: double, Volume: int, MarketCap: bigint, EnterpriseValue: bigint, PeRatio: double, PsRatio: double, PbRatio: double, EnterprisesValueRevenueRatio: double, EnterprisesValueEBITDARatio: double, TotalRevenue: bigint, OperatingRevenue: bigint, CostOfRevenue: bigint, GrossProfit: bigint, OperatingExpense: bigint, SellingGeneralAndAdministration: bigint, OperatingIncome: bigint, OtherIncomeExpense: bigint, PretaxIncome: bigint, TaxProvision: bigint, NetIncomeCommonStockholders: bigint, NetIncome: bigint, NetIncomeIncludingNoncontrollingInterests: bigint, NetIncomeContinuousOperations: bigint, DilutedNIAvailtoComStockholders: bigint, BasicEPS: double, DilutedEPS: double, BasicAverageShares: bigint, DilutedAverageShares: bigint, TotalOperatingIncomeAsReported: bigint, TotalExpenses: bigint, NetIncomeFromContinuingAndDiscontinuedOperation: bigint, NormalizedIncome: bigint, EBIT: bigint, EBITDA: bigint, ReconciledCostOfRevenue: bigint, ReconciledDepreciation: int, NetIncomeFromContinuingOperationNetMinorityInterest: bigint, NormalizedEBITDA: bigint, TaxRateForCalcs: double, TaxEffectOfUnusualItems: string, TotalAssets: bigint, CurrentAssets: bigint, CashCashEquivalentsAndShortTermInvestments: bigint, CashAndCashEquivalents: bigint, Receivables: bigint, Inventory: bigint, TotalNonCurrentAssets: bigint, NetPPE: bigint, GrossPPE: bigint, AccumulatedDepreciation: bigint, GoodwillAndOtherIntangibleAssets: bigint, OtherIntangibleAssets: bigint, OtherNonCurrentAssets: bigint, TotalLiabilitiesNetMinorityInterest: bigint, CurrentLiabilities: bigint, PayablesAndAccruedExpenses: bigint, Payables: bigint, AccountsPayable: bigint, CurrentDebtAndCapitalLeaseObligation: bigint, CurrentDebt: bigint, TotalNonCurrentLiabilitiesNetMinorityInterest: bigint, LongTermDebtAndCapitalLeaseObligation: bigint, LongTermDebt: bigint, NonCurrentDeferredLiabilities: bigint, NonCurrentDeferredTaxesLiabilities: bigint, OtherNonCurrentLiabilities: bigint, TotalEquityGrossMinorityInterest: bigint, StockholdersEquity: bigint, RetainedEarnings: bigint, TotalCapitalization: bigint, CommonStockEquity: bigint, NetTangibleAssets: bigint, WorkingCapital: bigint, InvestedCapital: bigint, TangibleBookValue: bigint, TotalDebt: bigint, ShareIssued: bigint, OrdinarySharesNumber: bigint, OperatingCashFlow: bigint, CashFlowFromContinuingOperatingActivities: bigint, NetIncomeFromContinuingOperations: bigint, DepreciationAmortizationDepletion: int, DepreciationAndAmortization: int, DeferredTax: int, DeferredIncomeTax: int, InvestingCashFlow: bigint, CashFlowFromContinuingInvestingActivities: bigint, NetPPEPurchaseAndSale: int, PurchaseOfPPE: int, FinancingCashFlow: bigint, CashFlowFromContinuingFinancingActivities: bigint, NetIssuancePaymentsOfDebt: bigint, NetCommonStockIssuance: int, CommonStockIssuance: int, CommonStockPayments: bigint, EndCashPosition: bigint, ChangesInCash: bigint, EffectOfExchangeRateChanges: int, BeginningCashPosition: bigint, CapitalExpenditure: int, IssuanceOfCapitalStock: int, RepurchaseOfCapitalStock: bigint, FreeCashFlow: bigint]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pyspark to add ROI and buy/sell columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/12/01 20:39:06 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "+----------+---------+------------------+----------+-------+-------+-------+------------+-----------+----------------------------+---------------------------+----------------+---------------+----------+--------+-------------+----------------+----------+----------+-----------+----------------------+-----------+-----------+-----------+----------------+\n",
      "|      Date|Adj Close|PercentChangeClose|Percent100|PeRatio|PsRatio|PbRatio|TotalRevenue|GrossProfit|EnterprisesValueRevenueRatio|EnterprisesValueEBITDARatio|OperatingExpense|OperatingIncome| NetIncome|BasicEPS|TotalExpenses|NormalizedIncome|      EBIT|    EBITDA|TotalAssets|CashAndCashEquivalents|     NetPPE|   GrossPPE|  TotalDebt|RetainedEarnings|\n",
      "+----------+---------+------------------+----------+-------+-------+-------+------------+-----------+----------------------------+---------------------------+----------------+---------------+----------+--------+-------------+----------------+----------+----------+-----------+----------------------+-----------+-----------+-----------+----------------+\n",
      "| 9/30/2023|56.490002|             0.919|     91.94| 23.228|  5.509|  9.305| 11953000000| 7296000000|                       22.42|                     63.883|      4026000000|     3270000000|3087000000|    0.71|   8683000000|      3087000000|3905000000|4195000000|97578000000|           11883000000| 8860000000|18048000000|40171000000|     73793000000|\n",
      "| 6/30/2023|61.442528|             0.973|    97.275| 26.529|  6.019| 10.326| 11972000000| 7060000000|                      24.098|                     81.611|      4659000000|     2401000000|2547000000|    0.59|   9571000000|      2547000000|3254000000|3535000000|98456000000|           12564000000| 9706000000|19233000000|41625000000|     72695000000|\n",
      "| 3/31/2023|63.164001|             1.054|   105.418| 28.324|  6.275|  11.13| 10980000000| 6663000000|                       26.94|                     62.789|      3296000000|     3367000000|3107000000|    0.72|   7613000000|      3107000000|4425000000|4711000000|97404000000|           12004000000| 9848000000|19241000000|42400000000|     72137000000|\n",
      "|12/31/2022|  59.9179|             1.032|   103.183| 27.899|  6.537| 12.072| 10125000000| 5612000000|                      29.773|                     96.898|      3537000000|     2075000000|2031000000|    0.47|   8050000000|      2031000000|2804000000|3111000000|92763000000|            9519000000| 9841000000|19075000000|39149000000|     71019000000|\n",
      "| 9/30/2022|58.069668|             0.939|    93.948| 25.464|  5.898| 10.529| 11063000000| 6497000000|                      24.633|                     69.008|      3409000000|     3088000000|2825000000|    0.65|   7975000000|      2825000000|3642000000|3949000000|92471000000|           10127000000| 9243000000|18235000000|39587000000|     70893000000|\n",
      "| 6/30/2022|61.810516|               1.0|   100.037| 26.544|  6.815| 10.977| 11325000000| 6495000000|                      26.849|                    108.439|      4154000000|     2341000000|1905000000|    0.44|   8984000000|      1905000000|2482000000|2804000000|93169000000|            8976000000| 9462000000|18561000000|41901000000|     69970000000|\n",
      "| 3/31/2022|61.787926|             1.067|   106.711| 27.556|  6.961| 11.686| 10491000000| 6400000000|                      28.492|                     75.405|      2995000000|     3405000000|2781000000|    0.64|   7086000000|      2781000000|3640000000|3964000000|94064000000|            7681000000| 9784000000|18888000000|41701000000|     69969000000|\n",
      "|12/31/2021|57.901939|             1.091|   109.071| 29.167|   6.79| 11.547|  9464000000| 5376000000|                      29.895|                     81.628|      3704000000|     1672000000|2414000000|    0.56|   7792000000|      2414000000|3125000000|3466000000|94354000000|            9684000000| 9920000000|18862000000|42761000000|     69094000000|\n",
      "| 9/30/2021|53.086636|             0.996|    99.589| 28.059|  6.239|  10.18| 10042000000| 6065000000|                      25.439|                     69.874|      3167000000|     2898000000|2471000000|    0.57|   7144000000|      2471000000|3294000000|3656000000|90606000000|           11301000000|10058000000|19211000000|41708000000|     68494000000|\n",
      "| 6/30/2021|53.305889|             1.064|   106.446| 32.401|  6.999| 11.462| 10129000000| 6342000000|                      26.232|                     55.574|      3326000000|     3016000000|2641000000|    0.61|   7113000000|      2641000000|4398000000|4781000000|90194000000|            9188000000|10547000000|19909000000|42008000000|     67838000000|\n",
      "| 3/31/2021|50.077721|              1.13|   113.041| 29.447|  6.902|  11.77|  9020000000| 5515000000|                      28.716|                     72.535|      2793000000|     2722000000|2245000000|    0.52|   6298000000|      2245000000|3205000000|3571000000|89993000000|            8484000000|10673000000|19859000000|44983000000|     67009000000|\n",
      "|12/31/2020|44.300453|              1.01|   100.973| 28.415|   7.08|  12.68|  8611000000| 5033000000|                      31.084|                     86.314|      2695000000|     2338000000|1456000000|    0.34|   6273000000|      1456000000|2671000000|3101000000|87296000000|            6795000000|10777000000|19700000000|42793000000|     66555000000|\n",
      "| 9/30/2020|43.873634|             1.026|   102.559| 23.288|  6.216| 12.134|  8652000000| 5181000000|                      28.278|                      76.48|      2883000000|     2298000000|1737000000|     0.4|   6354000000|      1737000000|2841000000|3199000000|97184000000|           11385000000|10667000000|19544000000|52867000000|     66863000000|\n",
      "| 6/30/2020|42.778782|             1.039|   103.877| 19.259|  5.191| 10.568| 15751000000| 9367000000|                      14.261|                     37.098|      5006000000|     4361000000|4554000000|    1.06|  11390000000|      4554000000|5674000000|6055000000|94689000000|           10037000000|10695000000|19234000000|52333000000|     66888000000|\n",
      "| 3/31/2020|41.182129|             0.793|    79.268| 21.377|  5.122| 10.011|  8601000000| 5230000000|                      25.764|                     62.072|      2850000000|     2380000000|2775000000|    0.65|   6221000000|      2775000000|3203000000|3570000000|94013000000|           13561000000|10993000000|19278000000|50393000000|     66870000000|\n",
      "|12/31/2019|51.953102|             1.081|   108.095| 29.441|  7.108|  12.66|  9068000000| 5502000000|                      29.523|                     87.632|      3338000000|     2164000000|2042000000|    0.48|   6904000000|      2042000000|2655000000|3055000000|86381000000|            6480000000|10838000000|18921000000|42763000000|     65810000000|\n",
      "| 9/30/2019|48.062347|             1.042|   104.176| 31.836|  7.135| 12.828|  9507000000| 5740000000|                      27.487|                     70.915|      3241000000|     2499000000|2593000000|    0.61|   7008000000|      2593000000|3322000000|3685000000|87433000000|            7531000000|10217000000|19794000000|42476000000|     65481000000|\n",
      "| 6/30/2019|46.135609|             1.081|   108.114| 31.049|  6.788| 12.274|  9997000000| 6076000000|                      25.135|                     69.565|      3088000000|     2988000000|2607000000|    0.61|   7009000000|      2607000000|3285000000|3612000000|89996000000|            6731000000|10254000000|19806000000|45075000000|     64602000000|\n",
      "| 3/31/2019|42.673019|             1.028|   102.822| 29.847|  6.324| 11.778|  8020000000| 5030000000|                      28.378|                     86.241|      2694000000|     2336000000|1678000000|    0.39|   5684000000|      1621000000|2364000000|2639000000|88347000000|            5645000000| 8866000000|18140000000|44267000000|     63704000000|\n",
      "|12/31/2018|41.501797|             1.013|   101.317| 64.863|  6.304| 11.065|  7058000000| 4337000000|                      32.315|                    221.652|      2701000000|     1636000000| 870000000|     0.2|   5422000000|       735000000|1029000000|1029000000|83216000000|            9077000000| 9598000000|17611000000|44214000000|     63234000000|\n",
      "+----------+---------+------------------+----------+-------+-------+-------+------------+-----------+----------------------------+---------------------------+----------------+---------------+----------+--------+-------------+----------------+----------+----------+-----------+----------------------+-----------+-----------+-----------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"SubsetColumns\").getOrCreate()\n",
    "\n",
    "# Define a list of columns you want to include in the subset\n",
    "selected_columns = [\"Date\", \"Adj Close\", \"PercentChangeClose\", \"Percent100\", \"PeRatio\", \"PsRatio\", \"PbRatio\",\"TotalRevenue\",\"GrossProfit\", \n",
    "                    \"EnterprisesValueRevenueRatio\", \"EnterprisesValueEBITDARatio\", \"OperatingExpense\", \n",
    "                    \"OperatingIncome\", \"NetIncome\", \"BasicEPS\", \"TotalExpenses\", \n",
    "                    \"NormalizedIncome\", \"EBIT\", \"EBITDA\", \"TotalAssets\", \"CashAndCashEquivalents\", \n",
    "                    \"NetPPE\", \"GrossPPE\", \"TotalDebt\", \"RetainedEarnings\"]\n",
    "\n",
    "# Use the select function to create a new DataFrame with the subset of columns\n",
    "new_df = data.select(selected_columns)\n",
    "\n",
    "# Show the new DataFrame\n",
    "new_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------------------+\n",
      "|PercentChangeClose|ROI                   |\n",
      "+------------------+----------------------+\n",
      "|0.919             |-0.08099999999999996  |\n",
      "|0.973             |-0.027000000000000024 |\n",
      "|1.054             |0.05400000000000005   |\n",
      "|1.032             |0.03200000000000003   |\n",
      "|0.939             |-0.061000000000000054 |\n",
      "|1.0               |0.0                   |\n",
      "|1.067             |0.06699999999999995   |\n",
      "|1.091             |0.09099999999999997   |\n",
      "|0.996             |-0.0040000000000000036|\n",
      "|1.064             |0.06400000000000006   |\n",
      "+------------------+----------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df = new_df.withColumn(\"ROI\", new_df[\"PercentChangeClose\"]-1)\n",
    "new_df.select(\"PercentChangeClose\",\"ROI\").show(10,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"StringToFloatConversion\").getOrCreate()\n",
    "\n",
    "# Assuming you have a DataFrame called \"your_data\"\n",
    "# Replace \"your_data\" with your actual DataFrame name\n",
    "data = data  # Replace this with your actual DataFrame\n",
    "\n",
    "# Get the list of columns in the DataFrame\n",
    "all_columns = data.columns\n",
    "\n",
    "# Iterate through each column and update its type to float\n",
    "for column in all_columns:\n",
    "    data = data.withColumn(column, col(column).cast(\"float\"))\n",
    "\n",
    "# Show the updated DataFrame\n",
    "data.describe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scalerModel = scaler.fit(train_data)\n",
    "scaledData = scalerModel.transform(train_data)\n",
    "\n",
    "scaledData.select(\"high_price\",\"features\",\"scaledFeatures\").show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [-1.3657523002323226,3.022907947488744,0.8298046014627691,2.159815264116857,-2.164905933739686,0.513864503418112,0.026467450651519845,-2.6470944424845335,-1.1731383459412668,-1.0379167916912122,0.13270185802545417,3.150346087909338,1.1641720697992026,1.5417004537956427,-0.14804725064384222,7.948201727660355,0.3595092088958347,-4.4186081919064915,-2.0635762874364416,2.299482385943349,8.639551338450365]\n",
      "Intercept: -5.618907933996588\n",
      "Coefficient Standard Errors: [0.38712650252711656, 1.159376615277703, 0.8158312513800045, 1.9412611781284872, 1.8940869071158155, 1.1560788717615322, 0.6195454298724, 1.5168093401572444, 1.281171122164314, 1.5616643933924805, 1.6387324608008342, 1.7930606960593278, 0.934605627745882, 1.5841006679091052, 1.6851046819693136, 1.8346258346305842, 0.9275385230192119, 1.407244013867867, 1.7682753598496104, 1.184247435578401, 1.655857178491178, 1.598798351274696]\n",
      "T Values: [-3.5279225041862317, 2.6073563220564644, 1.0171277455468006, 1.1125835557063328, -1.1429813096782635, 0.44448913994520983, 0.04272075843892677, -1.7451728258807497, -0.9156765444099733, -0.6646221788002058, 0.08097835442925438, 1.7569656703941834, 1.2456292100519424, 0.9732338891256024, -0.08785641166863616, 4.332328465908029, 0.3875949084309766, -3.1399019277131393, -1.166999401955104, 1.9417246065812706, 5.217570362150889, -3.514456922923847]\n",
      "P Values: [0.000681840163709424, 0.010795585390354745, 0.31201334102836387, 0.2690612438959572, 0.25629354341650834, 0.6578320940090812, 0.9660255178808996, 0.08461175097829154, 0.36245783300930445, 0.5081126659755546, 0.9356517971893132, 0.08256754786346177, 0.21636383491786582, 0.3332313730863854, 0.9301999124710707, 4.064041877649416e-05, 0.6992962348038103, 0.0023328992693225636, 0.24651285392954891, 0.05552254227734443, 1.2861093297011905e-06, 0.0007126530242416163]\n",
      "Dispersion: 10.26030301655104\n",
      "Null Deviance: 24221.369043522598\n",
      "Residual Degree Of Freedom Null: 105\n",
      "Deviance: 861.8654533902874\n",
      "Residual Degree Of Freedom: 84\n",
      "AIC: 568.9549372877447\n",
      "Deviance Residuals: \n",
      "+--------------------+\n",
      "|   devianceResiduals|\n",
      "+--------------------+\n",
      "| -0.4784381165089757|\n",
      "|  0.5287254232472356|\n",
      "| 0.41085006675383173|\n",
      "| 0.38413318510057604|\n",
      "|  1.3525600130951645|\n",
      "|-0.01708320225543...|\n",
      "| -2.2644040494743543|\n",
      "| -2.4774121244148333|\n",
      "|  1.6041771018393796|\n",
      "| -1.1529556505587184|\n",
      "|   -2.38822510702923|\n",
      "|  -1.657233285143242|\n",
      "|  1.2923781438029192|\n",
      "| 0.38934439014893485|\n",
      "| -1.6344448348371579|\n",
      "| -0.3722814655475801|\n",
      "| -0.8221132129940223|\n",
      "|  0.3049475679360505|\n",
      "| -3.4240498119798346|\n",
      "|  0.6212533992519624|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "#from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Create a Spark session\n",
    "#spark = SparkSession.builder.appName(\"StockPricePrediction\").getOrCreate()\n",
    "\n",
    "# Assuming your data is in a DataFrame called \"data\"\n",
    "# Replace \"target_column\" with the actual column name of your target variable\n",
    "target_column = \"Adj Close\"\n",
    "features_columns = [\"PeRatio\", \"PsRatio\", \"PbRatio\",\"TotalRevenue\",\"GrossProfit\", \n",
    "                    \"EnterprisesValueRevenueRatio\", \"EnterprisesValueEBITDARatio\", \"OperatingExpense\", \n",
    "                    \"OperatingIncome\", \"NetIncome\", \"BasicEPS\", \"TotalExpenses\", \n",
    "                    \"NormalizedIncome\", \"EBIT\", \"EBITDA\", \"TotalAssets\", \"CashAndCashEquivalents\", \n",
    "                    \"NetPPE\", \"GrossPPE\", \"TotalDebt\", \"RetainedEarnings\"]\n",
    "\n",
    "# Assemble the features into a vector\n",
    "assembler = VectorAssembler(inputCols=features_columns, outputCol=\"features\")\n",
    "data_assembled = assembler.transform(new_df)\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scalerModel = scaler.fit(data_assembled)\n",
    "scaledData = scalerModel.transform(data_assembled)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "(train_data, test_data) = scaledData.randomSplit([0.8, 0.2], seed=3)\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "lr = GeneralizedLinearRegression(labelCol=target_column, featuresCol=\"scaledFeatures\",\n",
    "                      family=\"gaussian\", link=\"identity\",\n",
    "                       maxIter=10,\n",
    "                      regParam=0.3) \n",
    "                      #elasticNetParam=0.8)\n",
    "\n",
    "# Create a pipeline with the assembler and the logistic regression model\n",
    "#pipeline = Pipeline(stages=[lr])\n",
    "\n",
    "# Train the model\n",
    "model = lr.fit(train_data)\n",
    "\n",
    "# Print the weights and intercept for linear regression\n",
    "print(\"Weights: \" + str(model.coefficients))\n",
    "print(\"Intercept: \" + str(model.intercept))\n",
    "\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "summary = model.summary\n",
    "print(\"Coefficient Standard Errors: \" + str(summary.coefficientStandardErrors))\n",
    "print(\"T Values: \" + str(summary.tValues))\n",
    "print(\"P Values: \" + str(summary.pValues))\n",
    "print(\"Dispersion: \" + str(summary.dispersion))\n",
    "print(\"Null Deviance: \" + str(summary.nullDeviance))\n",
    "print(\"Residual Degree Of Freedom Null: \" + str(summary.residualDegreeOfFreedomNull))\n",
    "print(\"Deviance: \" + str(summary.deviance))\n",
    "print(\"Residual Degree Of Freedom: \" + str(summary.residualDegreeOfFreedom))\n",
    "print(\"AIC: \" + str(summary.aic))\n",
    "print(\"Deviance Residuals: \")\n",
    "summary.residuals().show()\n",
    "\n",
    "\n",
    "\n",
    "# Make predictions on the test set\n",
    "#predictions = model.transform(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+--------------------+--------------------+\n",
      "|             Feature|               Coeff|Coefficient Std Err|             P-value|             T-value|\n",
      "+--------------------+--------------------+-------------------+--------------------+--------------------+\n",
      "|             PeRatio| -1.3657523002323226|0.38712650252711656| 6.81840163709424E-4| -3.5279225041862317|\n",
      "|             PsRatio|   3.022907947488744|  1.159376615277703|0.010795585390354745|  2.6073563220564644|\n",
      "|             PbRatio|  0.8298046014627691| 0.8158312513800045| 0.31201334102836387|  1.0171277455468006|\n",
      "|        TotalRevenue|   2.159815264116857| 1.9412611781284872|  0.2690612438959572|  1.1125835557063328|\n",
      "|         GrossProfit|  -2.164905933739686| 1.8940869071158155| 0.25629354341650834| -1.1429813096782635|\n",
      "|EnterprisesValueR...|   0.513864503418112| 1.1560788717615322|  0.6578320940090812| 0.44448913994520983|\n",
      "|EnterprisesValueE...|0.026467450651519845|    0.6195454298724|  0.9660255178808996| 0.04272075843892677|\n",
      "|    OperatingExpense| -2.6470944424845335| 1.5168093401572444| 0.08461175097829154| -1.7451728258807497|\n",
      "|     OperatingIncome| -1.1731383459412668|  1.281171122164314| 0.36245783300930445| -0.9156765444099733|\n",
      "|           NetIncome| -1.0379167916912122| 1.5616643933924805|  0.5081126659755546| -0.6646221788002058|\n",
      "|            BasicEPS| 0.13270185802545417| 1.6387324608008342|  0.9356517971893132| 0.08097835442925438|\n",
      "|       TotalExpenses|   3.150346087909338| 1.7930606960593278| 0.08256754786346177|  1.7569656703941834|\n",
      "|    NormalizedIncome|  1.1641720697992026|  0.934605627745882| 0.21636383491786582|  1.2456292100519424|\n",
      "|                EBIT|  1.5417004537956427| 1.5841006679091052|  0.3332313730863854|  0.9732338891256024|\n",
      "|              EBITDA|-0.14804725064384222| 1.6851046819693136|  0.9301999124710707|-0.08785641166863616|\n",
      "|         TotalAssets|   7.948201727660355| 1.8346258346305842|4.064041877649416E-5|   4.332328465908029|\n",
      "|CashAndCashEquiva...|  0.3595092088958347| 0.9275385230192119|  0.6992962348038103|  0.3875949084309766|\n",
      "|              NetPPE| -4.4186081919064915|  1.407244013867867|0.002332899269322...| -3.1399019277131393|\n",
      "|            GrossPPE| -2.0635762874364416| 1.7682753598496104| 0.24651285392954891|  -1.166999401955104|\n",
      "|           TotalDebt|   2.299482385943349|  1.184247435578401| 0.05552254227734443|  1.9417246065812706|\n",
      "|    RetainedEarnings|   8.639551338450365|  1.655857178491178|1.286109329701190...|   5.217570362150889|\n",
      "+--------------------+--------------------+-------------------+--------------------+--------------------+\n",
      "\n",
      "+----------------+-------------------+-------------------+--------------------+-------------------+\n",
      "|         Feature|              Coeff|Coefficient Std Err|             P-value|            T-value|\n",
      "+----------------+-------------------+-------------------+--------------------+-------------------+\n",
      "|         PeRatio|-1.3657523002323226|0.38712650252711656| 6.81840163709424E-4|-3.5279225041862317|\n",
      "|         PsRatio|  3.022907947488744|  1.159376615277703|0.010795585390354745| 2.6073563220564644|\n",
      "|     TotalAssets|  7.948201727660355| 1.8346258346305842|4.064041877649416E-5|  4.332328465908029|\n",
      "|          NetPPE|-4.4186081919064915|  1.407244013867867|0.002332899269322...|-3.1399019277131393|\n",
      "|RetainedEarnings|  8.639551338450365|  1.655857178491178|1.286109329701190...|  5.217570362150889|\n",
      "+----------------+-------------------+-------------------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Extract p-values or t-values for each feature\n",
    "p_values = summary.pValues\n",
    "t_values = summary.tValues\n",
    "\n",
    "coefficients_double = [float(x) for x in model.coefficients]\n",
    "\n",
    "# Create a DataFrame with feature names, coefficients, p-values, and t-values\n",
    "feature_importance_df = spark.createDataFrame(list(zip(features_columns, coefficients_double, summary.coefficientStandardErrors, p_values, t_values)),\n",
    "                                             [\"Feature\", \"Coeff\", \"Coefficient Std Err\", \"P-value\", \"T-value\"])\n",
    "\n",
    "# Show the feature importance DataFrame\n",
    "feature_importance_df.show(100)\n",
    "\n",
    "# You can filter the DataFrame based on p-values or t-values to identify significant features\n",
    "significant_features = feature_importance_df.filter(F.col(\"P-value\") < 0.05)\n",
    "\n",
    "# Show significant features\n",
    "significant_features.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.describe of DataFrame[Feature: string, Coefficient Std Err: double, P-value: double, T-value: double]>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaledData2.select(\"features\",\"scaledFeatures\").show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]\n",
      "Intercept: 0.044452830188679245\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "#from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Create a Spark session\n",
    "#spark = SparkSession.builder.appName(\"StockPricePrediction\").getOrCreate()\n",
    "\n",
    "# Assuming your data is in a DataFrame called \"data\"\n",
    "# Replace \"target_column\" with the actual column name of your target variable\n",
    "target_column2 = \"ROI\"\n",
    "features_columns2 = [\"PeRatio\", \"PsRatio\", \"PbRatio\",\"TotalRevenue\",\"GrossProfit\", \n",
    "                    \"EnterprisesValueRevenueRatio\", \"EnterprisesValueEBITDARatio\", \"OperatingExpense\", \n",
    "                    \"OperatingIncome\", \"NetIncome\", \"BasicEPS\", \"TotalExpenses\", \n",
    "                    \"NormalizedIncome\", \"EBIT\", \"EBITDA\", \"TotalAssets\", \"CashAndCashEquivalents\", \n",
    "                    \"NetPPE\", \"GrossPPE\", \"TotalDebt\", \"RetainedEarnings\"]\n",
    "\n",
    "# Assemble the features into a vector\n",
    "assembler2 = VectorAssembler(inputCols=features_columns2, outputCol=\"features\")\n",
    "data_assembled2 = assembler2.transform(new_df)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "(train_data2, test_data2) = data_assembled2.randomSplit([0.8, 0.2], seed=3)\n",
    "\n",
    "scaler2 = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scalerModel2 = scaler2.fit(train_data2)\n",
    "scaledData2 = scalerModel2.transform(train_data2)\n",
    "\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "lr2 = LinearRegression(labelCol=target_column2, featuresCol=\"scaledFeatures\",\n",
    "                       maxIter=10,\n",
    "                      regParam=0.3, \n",
    "                      elasticNetParam=0.8)\n",
    "\n",
    "# Create a pipeline with the assembler and the logistic regression model\n",
    "#pipeline = Pipeline(stages=[lr])\n",
    "\n",
    "# Train the model\n",
    "model2 = lr2.fit(scaledData2)\n",
    "\n",
    "# Print the weights and intercept for linear regression\n",
    "print(\"Weights: \" + str(model2.coefficients))\n",
    "print(\"Intercept: \" + str(model2.intercept))\n",
    "\n",
    "# Make predictions on the test set\n",
    "#predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate the model using a binary classification evaluator\n",
    "#evaluator = BinaryClassificationEvaluator(labelCol=target_column)\n",
    "#accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "#print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Optionally, you can print the coefficients of the logistic regression model\n",
    "#print(f\"Coefficients: {model.stages[-1].coefficients}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------------------+-------------------+\n",
      "|      Date|Adj Close|PercentChangeClose|         prediction|\n",
      "+----------+---------+------------------+-------------------+\n",
      "|12/31/1991| 4.564611|             1.167|  5.229056685695257|\n",
      "|12/31/1995| 9.436478|             1.052|  9.401863230456234|\n",
      "|12/31/2000|15.285267|             0.963|  15.21749241489073|\n",
      "|12/31/2001|11.702862|             0.917|  13.57065186403396|\n",
      "|12/31/2002|11.000153|             0.874| 12.920112308922137|\n",
      "|12/31/2007|18.112333|              0.96| 16.149088025794015|\n",
      "|12/31/2008|13.494187|             0.978|  13.97904493704929|\n",
      "|12/31/2013|27.753237|             0.962|  32.58219526036381|\n",
      "|12/31/2015|33.519585|             1.021|  36.04211018190424|\n",
      "|12/31/2021|57.901939|             1.091| 45.687912149534554|\n",
      "| 3/31/1990| 2.179467|             1.113|0.22579803873050874|\n",
      "| 3/31/1993| 4.719271|             0.933|  5.484383010348122|\n",
      "| 3/31/1996|10.234785|             1.085|   11.4571685025214|\n",
      "| 3/31/2000|12.337753|             0.826| 11.158446114430982|\n",
      "| 3/31/2001|12.216757|             0.799| 13.627546997807608|\n",
      "| 3/31/2003|11.048383|             1.004| 13.188604262363741|\n",
      "| 3/31/2012|26.670477|             1.138| 28.630908731377282|\n",
      "| 3/31/2014|30.169849|             1.087| 33.325216153646466|\n",
      "| 3/31/2020|41.182129|             0.793| 47.185475752384555|\n",
      "| 3/31/2022|61.787926|             1.067|  47.65458491590126|\n",
      "| 3/31/2023|63.164001|             1.054|  48.28271476853633|\n",
      "| 6/30/1995| 8.163978|             1.133|  7.423218577237037|\n",
      "| 6/30/2002|13.464307|             0.903|  17.25369899476332|\n",
      "| 6/30/2004| 12.28904|             0.872| 15.752019382279714|\n",
      "| 6/30/2007| 15.81362|             1.005|  16.61108209878435|\n",
      "| 6/30/2008|16.016499|             0.881|  16.20917520935167|\n",
      "| 6/30/2014|29.278954|              0.97|  36.07979827851882|\n",
      "| 6/30/2017|37.595921|             1.071| 42.095828599455274|\n",
      "| 9/30/2000|15.866415|             0.988| 13.759138915854047|\n",
      "| 9/30/2012|26.340805|             0.927| 29.486349555385836|\n",
      "| 9/30/2019|48.062347|             1.042|  44.83474512714027|\n",
      "| 9/30/2020|43.873634|             1.026|  49.12475773667997|\n",
      "| 9/30/2022|58.069668|             0.939|   44.7176502671353|\n",
      "+----------+---------+------------------+-------------------+\n",
      "\n",
      "METRICS\n",
      "Mean Squared Error: 30.307733917309413\n",
      "R Squared: 0.9020613795653223\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# compute predictions. this will append column \"prediction\" to dataframe\n",
    "lrPred = model.transform(test_data)\n",
    "lrPred = lrPred.withColumn(\"prediction\", F.abs(lrPred[\"prediction\"]))\n",
    "lrPred.select(\"Date\",\"Adj Close\",\"PercentChangeClose\",\"prediction\").show(100)\n",
    "\n",
    "ev = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"Adj Close\")\n",
    "\n",
    "print(\"METRICS\")\n",
    "print(\"Mean Squared Error:\", ev.evaluate(lrPred, {ev.metricName: \"mse\"}))\n",
    "print(\"R Squared:\", ev.evaluate(lrPred, {ev.metricName:'r2'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+----------+------------------+\n",
      "|Adj Close|PercentChangeClose|Percent100|        prediction|\n",
      "+---------+------------------+----------+------------------+\n",
      "| 1.957785|             0.956|    95.579|1.0308207547169812|\n",
      "|   2.6057|             1.196|   119.557|1.0308207547169812|\n",
      "| 4.564611|             1.167|    116.73|1.0308207547169812|\n",
      "| 4.719271|             0.933|    93.283|1.0308207547169812|\n",
      "| 4.833506|             0.973|    97.327|1.0308207547169812|\n",
      "| 5.089425|             1.026|   102.629|1.0308207547169812|\n",
      "| 5.205487|             1.103|   110.303|1.0308207547169812|\n",
      "| 7.204547|             1.112|   111.155|1.0308207547169812|\n",
      "|  8.97204|             1.099|   109.898|1.0308207547169812|\n",
      "|11.702862|             0.917|    91.718|1.0308207547169812|\n",
      "+---------+------------------+----------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "METRICS\n",
      "Mean Squared Error: 0.008227474095709678\n",
      "R Squared: -0.0005303524442674679\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# compute predictions. this will append column \"prediction\" to dataframe\n",
    "lrPred2 = model2.transform(test_data)\n",
    "lrPred2 = lrPred2.withColumn(\"prediction\", F.abs(lrPred2[\"prediction\"]))\n",
    "lrPred2.select(\"Adj Close\",\"PercentChangeClose\",\"Percent100\",\"prediction\").show(10)\n",
    "\n",
    "ev2 = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"PercentChangeClose\")\n",
    "\n",
    "print(\"METRICS\")\n",
    "print(\"Mean Squared Error:\", ev2.evaluate(lrPred2, {ev2.metricName: \"mse\"}))\n",
    "print(\"R Squared:\", ev2.evaluate(lrPred2, {ev2.metricName:'r2'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scalerModel = scaler.fit(train_data)\n",
    "scaledData = scalerModel.transform(train_data)\n",
    "\n",
    "scaledData.select(\"high_price\",\"features\",\"scaledFeatures\").show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# instantiate the model\n",
    "lr = LogisticRegression(labelCol='high_price',\n",
    "                        featuresCol='scaledFeatures',\n",
    "                        maxIter=10, \n",
    "                        regParam=0.3, \n",
    "                        elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(scaledData)\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# compute predictions. this will append column \"prediction\" to dataframe\n",
    "lrPred = lrModel.transform(scaledData)\n",
    "lrPred.select('probability','prediction').show(5,truncate=False)\n",
    "\n",
    "# set up evaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",\n",
    "                                          labelCol=\"high_price\",\n",
    "                                          metricName=\"areaUnderPR\")\n",
    "\n",
    "# pass to evaluator the DF with predictions, labels\n",
    "aupr = evaluator.evaluate(lrPred)\n",
    "\n",
    "print(\"Area under PR Curve:\", aupr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS5110 Spark 3.3",
   "language": "python",
   "name": "ds5110_spark3.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
